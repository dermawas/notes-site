---
title: The "Context Capsule" That Crumbled – A Week Vibe Coding Our Website
date: 2025-11-10 13:25:00 +07:00
categories:
  - notes
tags:
  - AI
description: vibe coding, AI, chatgpt 5
toc: false
published: true
advanced:
  layout: single
  author_profile: false
---



Another week in the lab, pushing the boundaries of what a self-writing website for POC can be. This time, the battle wasn't with the code itself, but with the stateful nature (or lack thereof) of the LLM. Specifically, working within the browser-based environment of ChatGPT-5 proved to be an exercise in memory management—or rather, a fight against amnesia.



### The Problem of Heavy Context



The sheer weight of a continuous coding conversation led to inevitable performance issues. The quick fix—clearing the browser cache—came with a catastrophic side effect: wiping out the conversation history. This immediately forced the model into a state of severe hallucination, as it lost all context for the existing code structure.

My proposed solution was the "Context Capsule": a `TXT` file detailing the project's current state, past work, and interaction rules, generated by the LLM itself. The idea was to export this knowledge and import it into a fresh, clean conversation, thereby preserving the continuity of the project.



### The Context Capsule Failed



Unfortunately, this was an attempt to patch a fundamental architectural flaw. The previous knowledge was not retained. Every new conversation was a complete restart, forcing me to repeatedly:

1. Re-train the model on its behavioral rules (e.g., "Do not hallucinate," "Provide full code blocks").
2. Re-supply the previous code for modification, essentially starting from scratch with every query.

This lack of reliable context resulted in losing key functionality within the website's CMS. While the feature to review and publish LLM-submitted articles was restored, the workflow was irrevocably changed—a high-cost, high-stress solution to a low-level memory issue.



### The Tyranny of Repetition



Beyond the technical hurdles, the need to constantly remind the model of its previous output and correct its occasional, dismissive tendency to call my supplied code "your code" became utterly tiresome. This isn't collaboration; it's custodial care.

Conclusion: The n8n workflow for article production is now functional again, but the content quality is suspect, having veered off-topic. The immediate next step is clear: bypass the conversational interface and integrate the ChatGPT API directly into n8n. This shifts content creation entirely to an automated, API-driven process, hopefully solving the context and quality issues simultaneously.

> Verdict: AI/LLMs excel at research and providing *solutions to single, small-scale issues*. However, their inherent lack of persistent environmental context means implementation at a project level still demands deep user expertise. Relying solely on the LLM's "knowledge" is problematic when you can't reliably discern right from wrong.
